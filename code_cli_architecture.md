üöÄ Core LLM-Driven Architectureüåê The Conversational Loop (The Engine)

The system operates on a continuous, conversational loop driven by a Worker Large Language Model (LLM). This loop enables dynamic, adaptive execution of complex tasks.-----üõ†Ô∏è Key Process Flow
Step
Component
Action Description
Data Format
1. Generation
Worker LLM
Repeatedly prompted to generate executable instructions.
JSON-Coded Actions
2. Execution
Executor Modules
Actions are parsed, validated, and executed step-by-step.
Shell Commands, File Edits
3. State Update
System Core
Plan statuses are updated, and all execution outputs are recorded.
Real-time Status Data
4. Feedback
Data Pipe
Outputs from the execution step are immediately fed back to the Worker LLM.
Summarized Context

-----‚ú® System Pillars (Mimicking Autopilot)

This continuous feedback loop creates a sophisticated, autopilot experience similar to advanced CLI tools (Claude Code, Gemini CLI, Codex CLI).
Adaptive Intelligence: The LLM uses the live execution output (Step 4) as the next prompt input (Step 1), allowing it to course-correct and make informed decisions on subsequent actions.
Structured Control: All actions are governed by defined Action Schemas, ensuring reliable and predictable execution within the system environment.
Modular Design: The architecture necessitates dedicated New Executor Modules for handling and interpreting the diverse set of JSON-coded commands.
Completion Protocol: The loop continues, with summaries passed between modules, until the task reaches a predetermined completion state.

